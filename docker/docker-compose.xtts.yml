services:
  xtts-teacher:
    build:
      context: ..
      dockerfile: docker/Dockerfile.xtts_teacher
    container_name: "${VOICE_ID:-voice1}_xtts_teacher"
    restart: unless-stopped

    # GPU passthrough configuration using CDI
    devices:
      - "nvidia.com/gpu=all"
    
    environment:
      VOICE_ID: "${VOICE_ID:-voice1}"
      XTTS_MODEL_NAME: "${XTTS_MODEL_NAME:-tts_models/multilingual/multi-dataset/xtts_v2}"
      XTTS_SPEAKER_WAVS: "${XTTS_SPEAKER_WAVS}"
      XTTS_LANGUAGE: "${XTTS_LANGUAGE:-en}"
      XTTS_DEVICE: "${XTTS_DEVICE:-cuda}"
      XTTS_NUM_REFERENCE_CLIPS: "${XTTS_NUM_REFERENCE_CLIPS:-3}"
      UVICORN_WORKERS: "${UVICORN_WORKERS:-3}"
      COQUI_TOS_AGREED: "1"  # Auto-accept Terms of Service for non-interactive use
      TTS_HOME: "/root"  # TTS will use /root/tts/ (where we mount the cache)
      # PyTorch memory optimization to reduce fragmentation and prevent CUDA OOM
      PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:True"
      # CDI GPU passthrough environment variables
      NVIDIA_VISIBLE_DEVICES: "all"
      NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
    
    command: ["sh", "-c", "uvicorn xtts_teacher_server:app --host 0.0.0.0 --port 9010 --workers $${UVICORN_WORKERS:-3}"]

    volumes:
      # Host directory with speaker reference WAVs
      - "${REFERENCE_AUDIO_DIR}:/speakers:ro"
      # Host directory with XTTS model cache (to avoid re-downloading)
      # When TTS_HOME=/root, TTS creates /root/tts/ for models
      # So we mount the host's tts subdirectory to /root/tts in the container
      # Read-write needed for TOS file, but model files are read-only from host perspective
      - "${MODEL_CACHE_DIR}/tts:/root/tts:rw"

    ports:
      - "${XTTS_PORT:-9010}:9010"

