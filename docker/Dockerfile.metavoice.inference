# syntax=docker/dockerfile:1.6
# MetaVoice teacher model (RTX 5080 / PyTorch nightly cu128)
#
# Based on YOUR original Dockerfile.metavoice.inference with minimal fixes:
#   1) Remove the bogus typing_extensions==4.11.0a1 (it doesn't exist)
#   2) Ensure typing_extensions is new enough to provide TypeIs (torch nightly needs it)
#   3) Do NOT apply --no-deps to everything; only use it for audiocraft (to avoid torch downgrade)
#   4) Keep your metavoice-src clone + serving.py patch flow intact

# --- Stage 1: CUDA + Python ---------------------------------------------------
# Use devel image for build stage (has CUDA headers), runtime for final
FROM nvidia/cuda:12.8.1-cudnn-devel-ubuntu22.04 AS base
# Put PyTorch's bundled cuDNN first to avoid version conflicts
# PyTorch comes with cuDNN 9.10.2, system CUDA has 9.8.0
ENV DEBIAN_FRONTEND=noninteractive \
    PYTHONUNBUFFERED=1 \
    CUDA_HOME=/usr/local/cuda \
    PATH=${CUDA_HOME}/bin:${PATH} \
    LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${CUDA_HOME}/targets/x86_64-linux/lib:${LD_LIBRARY_PATH} \
    CPLUS_INCLUDE_PATH=${CUDA_HOME}/include:${CUDA_HOME}/targets/x86_64-linux/include:${CPLUS_INCLUDE_PATH} \
    C_INCLUDE_PATH=${CUDA_HOME}/include:${CUDA_HOME}/targets/x86_64-linux/include:${C_INCLUDE_PATH}

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        python3.10 python3.10-dev python3-pip \
        ffmpeg git git-lfs curl ca-certificates && \
    ln -sf /usr/bin/python3.10 /usr/bin/python && \
    pip install --no-cache-dir --upgrade pip setuptools wheel && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

# --- Stage 2: Python deps -----------------------------------------------------
FROM base AS deps
# build-time libs for PyAV (used by AudioCraft) and C++ compiler for xFormers
# CUDA headers are already in devel image
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        pkg-config \
        build-essential \
        g++ \
        cmake \
        libavdevice-dev libavformat-dev libavfilter-dev \
        libavcodec-dev  libavutil-dev  \
        libswresample-dev libswscale-dev \
        libsndfile1 && \
    rm -rf /var/lib/apt/lists/*

# 5080 requires sm_120 → use nightly cu128 wheels
RUN pip install --no-cache-dir \
        --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \
        torch==2.10.0.dev20251213+cu128 \
        torchvision==0.25.0.dev20251213+cu128 \
        torchaudio==2.10.0.dev20251213+cu128

# Fix: torch nightlies import TypeIs from typing_extensions (needs >=4.12.0).
# Your log showed typing_extensions in the container was too old.
RUN pip install --no-cache-dir typing_extensions==4.15.0

# Remove system cuDNN 9.8.0 libraries to force PyTorch to use its bundled cuDNN 9.10.2
# PyTorch was compiled against cuDNN 9.10.2, but system has 9.8.0 which causes version mismatch
# We delete the actual library files and symlinks so they won't be found by the dynamic linker
RUN rm -f /lib/x86_64-linux-gnu/libcudnn.so* && \
    rm -f /usr/lib/x86_64-linux-gnu/libcudnn.so* && \
    rm -f /etc/alternatives/libcudnn* 2>/dev/null || true

# Update LD_LIBRARY_PATH to prioritize PyTorch's bundled cuDNN (9.10.2)
# This ensures PyTorch can find its bundled cuDNN if it's in a separate location
ENV LD_LIBRARY_PATH=/usr/local/lib/python3.10/dist-packages/torch/lib:${LD_LIBRARY_PATH}

# Your runtime deps (install with deps enabled)
RUN pip install --no-cache-dir \
        transformers==4.37.0 \
        soundfile==0.12.1 \
        sentencepiece==0.1.99 \
        accelerate==0.27.2 \
        fastapi \
        uvicorn[standard] \
        tyro \
        attrs==23.2.0 \
        librosa \
        julius \
        av \
        omegaconf \
        einops \
        ninja \
        flashy \
        num2words \
        spacy \
        lightning_utilities \
        tiktoken \
        deepfilternet \
        pydub \
        posthog \
        python-multipart

# AudioCraft: install without letting it “helpfully” pin torch==2.1.0.
RUN pip install --no-cache-dir --no-deps \
        git+https://github.com/facebookresearch/audiocraft.git@main


# These have to be done after audiocraft and torch, otherwise they will be downgraded
RUN pip install --no-cache-dir --no-deps \
        torchdiffeq \
        torchmetrics

# Install xformers with patch to bypass PyTorch version check for dev versions
# xFormers rejects PyTorch dev versions, so we just comment out the entire if block

ENV CUDA_HOME=/usr/local/cuda \
    CUDA_PATH=/usr/local/cuda \
    CUDACXX=/usr/local/cuda/bin/nvcc \
    CPATH=/usr/local/cuda/include:/usr/local/cuda/targets/x86_64-linux/include \
    C_INCLUDE_PATH=/usr/local/cuda/include:/usr/local/cuda/targets/x86_64-linux/include \
    CPLUS_INCLUDE_PATH=/usr/local/cuda/include:/usr/local/cuda/targets/x86_64-linux/include

RUN git clone --depth 1 https://github.com/facebookresearch/xformers.git /tmp/xformers && \
    cd /tmp/xformers && \
    git submodule update --init --recursive && \
    sed -i '/if torch\.__version__ < "2\.10":/,/^[[:space:]]*)$/s/^[[:space:]]*/        # /' setup.py && \
    USE_CUDA=1 TORCH_CUDA_ARCH_LIST=12.0 \
    pip install -v --no-build-isolation -U . && \
    rm -rf /tmp/xformers

# Sanity check at build-time: use getattr to safely check version (handles cases where __version__ may not be exposed)
RUN python -c "import typing_extensions as te; from typing_extensions import TypeIs; import torch; version = getattr(te, '__version__', 'unknown'); print('typing_extensions', version, 'torch', torch.__version__)"

# torchaudio nightly no longer ships `torchaudio.backend`;
# keep your original shim (NO line-continuation backslashes here; heredocs are picky)
RUN python - <<'PY'
import pathlib, textwrap, importlib.util
ta = importlib.util.find_spec("torchaudio").submodule_search_locations[0]
backend = pathlib.Path(ta) / "backend"
backend.mkdir(exist_ok=True)
(backend/"__init__.py").write_text("from .common import AudioMetaData\n")
(backend/"common.py").write_text(textwrap.dedent('''
    class AudioMetaData:
        def __init__(self,*a,**kw): pass
'''))
PY

# --- Stage 3: Application -----------------------------------------------------
FROM deps AS app
WORKDIR /opt/metavoice
RUN git clone --depth 1 https://github.com/metavoiceio/metavoice-src.git . && \
    git lfs pull

# Patch torch.load default (PyTorch 2.6+)
RUN sed -i "1i\
import torch, builtins\n\
_load=torch.load\n\
def _pl(*a, **kw):\n\
    kw.setdefault('weights_only', False)\n\
    return _load(*a, **kw)\n\
torch.load=_pl\n" serving.py

# Fix bug in serving.py: change 'from attr[s] import dataclass' to Python's built-in 'from dataclasses import dataclass'
RUN sed -i -e 's/from attr import dataclass/from dataclasses import dataclass/' -e 's/from attrs import dataclass/from dataclasses import dataclass/' serving.py

# Fix dtype mismatch in fast_model.py: ensure query tensor matches key/value dtype (float16)
# The error shows query is float32 but key/value are float16, causing scaled_dot_product_attention to fail
RUN sed -i 's/y = F\.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0\.0)/y = F.scaled_dot_product_attention(q.to(k.dtype), k, v, attn_mask=mask, dropout_p=0.0)/' fam/llm/fast_model.py

# Fix dtype mismatch: ensure model and spk_emb are converted to float16, encoded stays int
# NOTE: encoded contains token indices (integers) and must NOT be converted to float16
# TorchDynamo may infer wrong dtype during compilation, so we explicitly ensure encoded is int
# Patch fast_inference_utils.py to convert model and spk_emb to float16 in TWO places:
# 1. In build_model() during initialization (for container startup)
# 2. In main() during actual inference (for API calls)
RUN python <<'PY'
with open('fam/llm/fast_inference_utils.py', 'r') as f:
    lines = f.readlines()

# First: Find the generate() call in build_model and insert model + spk_emb conversion before it
inserted_build = False
for i, line in enumerate(lines):
    if not inserted_build and 'def build_model' in line:
        # We're in build_model function, look for the generate() call
        for j in range(i, min(i + 400, len(lines))):
            if 'generate(' in lines[j] and ('y =' in lines[j] or '=' in lines[j]):
                # Found the generate call, insert model and spk_emb conversion before it
                indent = len(lines[j]) - len(lines[j].lstrip())
                # Convert model to float16
                lines.insert(j, ' ' * indent + '# Convert model and spk_emb to float16 to avoid dtype mismatches (initialization)\n')
                lines.insert(j + 1, ' ' * indent + '# NOTE: encoded contains token indices (integers) and must remain int\n')
                lines.insert(j + 2, ' ' * indent + 'model = model.half()\n')
                # Convert spk_emb to float16 to match model dtype
                lines.insert(j + 3, ' ' * indent + 'spk_emb = spk_emb.half()\n')
                # Explicitly ensure encoded is integer dtype (torch.long) to prevent TorchDynamo from inferring float16
                lines.insert(j + 4, ' ' * indent + 'encoded = encoded.to(torch.long)\n')
                inserted_build = True
                break
        if inserted_build:
            break

# Second: Find the generate() call in main() and insert spk_emb conversion before it (for runtime inference)
# The model is already float16 from initialization, but spk_emb might be bfloat16 from the API call
inserted_main = False
for i, line in enumerate(lines):
    if not inserted_main and 'def main(' in line:
        # We're in main function, look for the generate() call
        for j in range(i, min(i + 100, len(lines))):
            # Look for 'y = generate(' pattern (can be on same line or split)
            if 'generate(' in lines[j]:
                # Check if this is the assignment (y = generate or y=generate)
                if 'y =' in lines[j] or 'y=' in lines[j] or (j > 0 and ('y =' in lines[j-1] or 'y=' in lines[j-1])):
                    # Found the generate call in main(), insert spk_emb conversion before it
                    indent = len(lines[j]) - len(lines[j].lstrip())
                    # Convert spk_emb to float16 to match model dtype (model is already float16 from initialization)
                    lines.insert(j, ' ' * indent + '# Convert spk_emb to float16 to match model dtype (runtime inference)\n')
                    lines.insert(j + 1, ' ' * indent + 'spk_emb = spk_emb.half()\n')
                    # Explicitly ensure encoded is integer dtype (torch.long)
                    lines.insert(j + 2, ' ' * indent + 'encoded = encoded.to(torch.long)\n')
                    inserted_main = True
                    break
        if inserted_main:
            break

# Fallback: If we didn't find generate in build_model, try to find load_state_dict
if not inserted_build:
    for i, line in enumerate(lines):
        if 'load_state_dict' in line and 'model' in line:
            # Found model loading, insert conversion after this statement completes
            base_indent = len(line) - len(line.lstrip())
            for j in range(i + 1, min(i + 10, len(lines))):
                next_line = lines[j]
                if next_line.strip() and not next_line.strip().startswith('#'):
                    next_indent = len(next_line) - len(next_line.lstrip())
                    if next_indent <= base_indent:
                        indent = base_indent
                        lines.insert(j, ' ' * indent + '# Convert model to float16 to avoid bfloat16/float16 dtype mismatches\n')
                        lines.insert(j + 1, ' ' * indent + 'model = model.half()\n')
                        inserted_build = True
                        break
            if inserted_build:
                break

with open('fam/llm/fast_inference_utils.py', 'w') as f:
    f.writelines(lines)
PY

# Ensure PyTorch's bundled cuDNN is used (9.10.2) instead of system cuDNN (9.8.0)
# This must be set at runtime, not just build time
ENV LD_LIBRARY_PATH=/usr/local/lib/python3.10/dist-packages/torch/lib:${LD_LIBRARY_PATH}

EXPOSE 9010
CMD [ "python", "serving.py", "--port=9010" ]
