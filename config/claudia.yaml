voice_id: claudia
language: en_GB
paths:
  input_audio_dir: input/claudia/audio
  workspace_dir: workspace/claudia
  models_dir: models/claudia
  shared_models_dir: models/shared
  corpus_path: input/shared/corpus.txt
asr:
  model_size: medium.en
  device: cuda
  beam_size: 5
  max_segment_seconds: 15
  min_segment_seconds: 1.0
  min_confidence: 0.7
phoneme_check:
  max_phoneme_distance: 0.1
  use_tts_roundtrip: true
synthetic:
  enabled: true
  corpus_text_path: input/shared/corpus.txt
  max_sentences: 3020
  tts_backend: http
  tts_http:
    base_url: http://localhost:9010/tts
    voice_id: claudia_clone
  teacher:
    kind: metavoice
    port: 9010
    model_name: tts_models/multilingual/multi-dataset/xtts_v2
    language: en
    device: cuda
    reference_audio_dir: workspace/claudia/datasets/real/wavs
    num_reference_clips: 0
    workers: 3  # With lock in place, 3 workers = ~10.5GB (3×2GB models + 3×1.5GB inference), fits within 12GB target
  # max_parallel_jobs is auto-calculated as workers * 2 (6 in this case)
  # You can override it by uncommenting the line below:
  # max_parallel_jobs: 12
training:
  base_checkpoint: cori-high-500-clean.ckpt
  batch_size: 16
  max_epochs: 700
  quality: high
  accelerator: gpu
  devices: 1
tms:
  enable_tts_dojo: true
  docker_compose_file: docker/docker-compose.training.yml
  project_name: claudia-voice
  expose_tensorboard: true
  tensorboard_port: 6006
